# Pytanie 14: Algorytmy i metody w systemach wieloagentowych i aktorowych

## Pytanie
**"WymieniÄ‡ i szczegÃ³Å‚owo opisaÄ‡ wybrane algorytmy i metody wykorzystywane w systemach wieloagentowych i aktorowych."**

Przedmiot: AASD (Agentowe i Aktorowe Systemy Decyzyjne)

---

## ğŸ“š OdpowiedÅº gÅ‚Ã³wna

### 1. Algorytmy negocjacji i aukcji

#### Contract Net Protocol (CNP)

```
Fazy:
1. ANNOUNCEMENT - Manager ogÅ‚asza zadanie (cfp)
2. BIDDING - Wykonawcy skÅ‚adajÄ… oferty (propose)
3. AWARDING - Manager wybiera najlepszÄ… (accept/reject)
4. EXECUTION - Wybrany wykonuje zadanie (inform)

        Manager                    Contractors
           â”‚                      â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
           â”‚â”€â”€â”€â”€â”€â”€ cfp â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ A â”‚ B â”‚ C â”‚
           â”‚                      â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
           â”‚â†â”€â”€â”€â”€â”€ propose â”€â”€â”€â”€â”€â”€â”€    â”‚   â”‚
           â”‚â†â”€â”€â”€â”€â”€ propose â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
           â”‚â†â”€â”€â”€â”€â”€ propose â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           â”‚
           â”‚â”€â”€ accept-proposal â”€â”€â†’ B (winner)
           â”‚â”€â”€ reject-proposal â”€â”€â†’ A, C
           â”‚
           â”‚â†â”€â”€â”€â”€â”€ inform â”€â”€â”€â”€â”€â”€â”€ B (result)
```

**Zastosowania:** PrzydziaÅ‚ zadaÅ„, zarzÄ…dzanie zasobami, e-commerce

#### Aukcje wieloagentowe

| Typ aukcji | Mechanizm | WÅ‚aÅ›ciwoÅ›ci |
|------------|-----------|-------------|
| **English** | RosnÄ…ce stawki | Otwarta, winner's curse |
| **Dutch** | MalejÄ…ce stawki | Szybka, ryzykowna |
| **First-price sealed** | ZapieczÄ™towane | Strategiczne zaniÅ¼anie |
| **Vickrey** | Second-price sealed | Truthful (incentive compatible) |

---

### 2. Algorytmy konsensusu

#### Raft (dla systemÃ³w aktorowych)

```
Stany wÄ™zÅ‚Ã³w: FOLLOWER â†’ CANDIDATE â†’ LEADER

Leader Election:
1. Follower timeout â†’ staje siÄ™ Candidate
2. Candidate wysyÅ‚a RequestVote do wszystkich
3. WiÄ™kszoÅ›Ä‡ gÅ‚osÃ³w â†’ nowy Leader
4. Leader wysyÅ‚a heartbeats

Log Replication:
1. Client â†’ Leader (command)
2. Leader â†’ Followers (AppendEntries)
3. WiÄ™kszoÅ›Ä‡ potwierdza â†’ commit
4. Leader â†’ Client (success)
```

#### PBFT (Practical Byzantine Fault Tolerance)

```
Toleruje f bÅ‚Ä™dnych wÄ™zÅ‚Ã³w przy n â‰¥ 3f + 1

Fazy:
PRE-PREPARE â†’ PREPARE â†’ COMMIT â†’ REPLY

Client â”€â”€requestâ”€â”€â†’ Primary
                      â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â†“          â†“          â†“
      Replica1   Replica2   Replica3
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
                   Reply
```

---

### 3. Algorytmy koordynacji

#### Distributed Mutual Exclusion

**Algorytm Ricarta-Agrawali:**
```
WejÅ›cie do sekcji krytycznej:
1. WyÅ›lij REQUEST(timestamp) do wszystkich
2. Czekaj na REPLY od wszystkich
3. WejdÅº do sekcji krytycznej

OdbiÃ³r REQUEST:
- JeÅ›li nie chcÄ™ CS lub mÃ³j timestamp > nadawcy â†’ wyÅ›lij REPLY
- W przeciwnym razie â†’ kolejkuj REQUEST
```

**ZÅ‚oÅ¼onoÅ›Ä‡:** 2(N-1) wiadomoÅ›ci na wejÅ›cie do CS

#### Token Ring
```
     â”Œâ”€â”€â”€â†’ A â”€â”€â”€â†’ B â”€â”€â”€â”
     â”‚                 â†“
     â”‚    [TOKEN]      â”‚
     â”‚                 â”‚
     â””â”€â”€â”€ D â†â”€â”€â”€ C â†â”€â”€â”€â”˜

Kto ma token â†’ moÅ¼e wejÅ›Ä‡ do CS
ZÅ‚oÅ¼onoÅ›Ä‡: 0 do N-1 wiadomoÅ›ci (Å›rednio N/2)
```

---

### 4. Algorytmy uczenia wieloagentowego

#### Q-Learning (Independent Learners)

```
KaÅ¼dy agent uczy siÄ™ niezaleÅ¼nie:

Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]

Problem: Åšrodowisko niestacjonarne (inni agenci siÄ™ zmieniajÄ…)
```

#### Nash Q-Learning

```
UwzglÄ™dnia akcje innych agentÃ³w:

Q_i(s, aâ‚, aâ‚‚, ..., aâ‚™) - wartoÅ›Ä‡ dla agenta i

Aktualizacja uÅ¼ywa rÃ³wnowagi Nasha:
Q_i â† Q_i + Î±[r_i + Î³ Nash(Q_i(s')) - Q_i]
```

#### Fictitious Play
```
1. Obserwuj akcje przeciwnikÃ³w
2. Buduj rozkÅ‚ad empiryczny ich strategii
3. Graj best response wobec tego rozkÅ‚adu
4. Powtarzaj

ZbieÅ¼noÅ›Ä‡ do Nasha w grach z wÅ‚aÅ›ciwoÅ›ciÄ… FP
```

---

### 5. Algorytmy dla aktorÃ³w

#### Supervision Strategies (Akka)

```scala
// One-for-One: restart tylko tego aktora
override val supervisorStrategy =
  OneForOneStrategy() {
    case _: ArithmeticException => Resume
    case _: NullPointerException => Restart
    case _: Exception => Stop
  }

// All-for-One: restart wszystkich dzieci
override val supervisorStrategy =
  AllForOneStrategy() {
    case _: Exception => Restart
  }
```

#### Routing Strategies

| Strategia | Opis |
|-----------|------|
| **Round Robin** | Po kolei do kaÅ¼dego |
| **Random** | Losowo |
| **Smallest Mailbox** | Do najmniej obciÄ…Å¼onego |
| **Broadcast** | Do wszystkich |
| **Consistent Hashing** | Wg klucza (locality) |

---

### 6. Algorytmy planowania (BDI)

#### Means-Ends Reasoning

```
Goal: be_at(destination)

Plans:
  plan1: walk(X,Y) :- distance(X,Y) < 1km
  plan2: drive(X,Y) :- have(car), distance(X,Y) >= 1km
  plan3: take_bus(X,Y) :- bus_available(X,Y)

WybÃ³r planu na podstawie:
- Kontekstu (beliefs)
- Preferencji
- Kosztu
```

#### Partial Order Planning (POP)

```
Cel: have(coffee) âˆ§ have(report)

       Start
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â†“         â†“
 buy(coffee) write(report)
    â”‚         â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â†“
        Goal

Ordering constraints: flexible (rÃ³wnolegÅ‚oÅ›Ä‡ gdy moÅ¼liwa)
```

---

### 7. Algorytmy formowania koalicji

#### Shapley Value

```
Sprawiedliwy podziaÅ‚ zyskÃ³w w koalicji:

Ï†áµ¢ = Î£ [|S|!(n-|S|-1)!/n!] Ã— [v(Sâˆª{i}) - v(S)]

Gdzie:
- S = podzbiÃ³r agentÃ³w bez i
- v(S) = wartoÅ›Ä‡ koalicji S
- n = liczba agentÃ³w

WÅ‚aÅ›ciwoÅ›ci: efektywnoÅ›Ä‡, symetria, addytywnoÅ›Ä‡
```

---

## ğŸ“Š Podsumowanie algorytmÃ³w

| Kategoria | Algorytm | Zastosowanie |
|-----------|----------|--------------|
| Negocjacje | Contract Net | PrzydziaÅ‚ zadaÅ„ |
| Aukcje | Vickrey | Truthful bidding |
| Konsensus | Raft, PBFT | Systemy rozproszone |
| Koordynacja | Ricart-Agrawala | Mutual exclusion |
| Uczenie | Nash Q-Learning | Gry wieloagentowe |
| Aktorzy | Supervision | Fault tolerance |
| Planowanie | Means-Ends | BDI agenty |
| Koalicje | Shapley Value | PodziaÅ‚ zyskÃ³w |

---

## ğŸ§  Mnemoniki

### "CNP = Call, Propose, Award, Execute":
Fazy Contract Net Protocol

### "RAFT = Replicated And Fault Tolerant":
Leader election + log replication

### "Vickrey = Second price = Truthful":
PÅ‚acisz drugÄ… cenÄ™ â†’ opÅ‚aca siÄ™ licytowaÄ‡ prawdziwie

---

## â“ Pytania dodatkowe

### Q1: "Dlaczego aukcja Vickreya jest truthful?"
**OdpowiedÅº:** PÅ‚acisz drugÄ… najwyÅ¼szÄ… cenÄ™, nie swojÄ…. Licytowanie poniÅ¼ej wartoÅ›ci = ryzyko przegranej. Licytowanie powyÅ¼ej = ryzyko przepÅ‚acenia. Optymalna strategia = licytuj prawdziwÄ… wartoÅ›Ä‡.

### Q2: "Jak Raft radzi sobie z partycjÄ… sieci?"
**OdpowiedÅº:** Tylko partycja z wiÄ™kszoÅ›ciÄ… wÄ™zÅ‚Ã³w moÅ¼e wybraÄ‡ lidera i commitowaÄ‡. MniejszoÅ›Ä‡ jest zablokowana (read-only lub niedostÄ™pna). Po naprawie partycji - synchronizacja logÃ³w.

### Q3: "Czym rÃ³Å¼ni siÄ™ One-for-One od All-for-One supervision?"
**OdpowiedÅº:** One-for-One: restart tylko wadliwego aktora (izolacja bÅ‚Ä™du). All-for-One: restart wszystkich dzieci (gdy stan jest wspÃ³Å‚dzielony/zaleÅ¼ny). WybÃ³r zaleÅ¼y od zaleÅ¼noÅ›ci miÄ™dzy aktorami.

---

## ğŸ¯ Kluczowe punkty

1. **Contract Net** = aukcja zadaÅ„ miÄ™dzy agentami
2. **Raft/PBFT** = konsensus w systemach rozproszonych
3. **Nash Q-Learning** = uczenie z uwzglÄ™dnieniem innych
4. **Supervision** = "let it crash" + automatic recovery
5. **Shapley Value** = sprawiedliwy podziaÅ‚ koalicji

---

## ğŸ“– Å¹rÃ³dÅ‚a

1. Shoham, Leyton-Brown - "Multiagent Systems"
2. Ongaro, Ousterhout - "In Search of an Understandable Consensus Algorithm" (Raft)
3. Akka Documentation - Supervision
4. Sandholm - "Distributed Rational Decision Making"
