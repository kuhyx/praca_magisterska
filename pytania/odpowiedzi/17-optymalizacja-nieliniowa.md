# Pytanie 17: Optymalizacja nieliniowa - warunki optymalnoÅ›ci

## Pytanie
**"PrzedstawiÄ‡ warunki konieczne i dostateczne optymalnoÅ›ci rÃ³Å¼niczkowalnych zadaÅ„ optymalizacji bez ograniczeÅ„ i z ograniczeniami oraz warunki regularnoÅ›ci i omÃ³wiÄ‡ metody poszukiwania rozwiÄ…zaÅ„ zadaÅ„ optymalizacji nieliniowej."**

Przedmiot: AMO (Analiza i Metody Optymalizacji)

---

## ğŸ“š OdpowiedÅº gÅ‚Ã³wna

### 1. Optymalizacja bez ograniczeÅ„

#### Problem
$$\min_{x \in \mathbb{R}^n} f(x)$$

#### Warunki konieczne (I rzÄ™du)
JeÅ›li $x^*$ jest minimum lokalnym i $f$ jest rÃ³Å¼niczkowalna:
$$\nabla f(x^*) = 0$$

(Gradient zerowy - punkt stacjonarny)

#### Warunki dostateczne (II rzÄ™du)
JeÅ›li $\nabla f(x^*) = 0$ oraz hesjan $H(x^*) = \nabla^2 f(x^*)$:

| Hesjan | Wniosek |
|--------|---------|
| $H \succ 0$ (dodatnio okreÅ›lony) | **Minimum lokalne** |
| $H \prec 0$ (ujemnie okreÅ›lony) | Maximum lokalne |
| $H$ nieokreÅ›lony | Punkt siodÅ‚owy |
| $H \succeq 0$ (pÃ³Å‚dodatni) | Brak wniosku |

**Sprawdzenie:** Wszystkie wartoÅ›ci wÅ‚asne $\lambda_i > 0 \Rightarrow H \succ 0$

---

### 2. Optymalizacja z ograniczeniami

#### Problem ogÃ³lny
$$\min_{x} f(x)$$
$$\text{s.t. } g_i(x) \leq 0, \quad i = 1, \ldots, m$$
$$\quad\quad h_j(x) = 0, \quad j = 1, \ldots, p$$

#### Lagrangian
$$L(x, \lambda, \mu) = f(x) + \sum_{i=1}^{m} \lambda_i g_i(x) + \sum_{j=1}^{p} \mu_j h_j(x)$$

---

### 3. Warunki KKT (Karush-Kuhn-Tucker)

#### Warunki konieczne I rzÄ™du

JeÅ›li $x^*$ jest minimum i speÅ‚nione sÄ… warunki regularnoÅ›ci:

1. **StacjonarnoÅ›Ä‡:**
$$\nabla_x L(x^*, \lambda^*, \mu^*) = 0$$

2. **DopuszczalnoÅ›Ä‡ pierwotna:**
$$g_i(x^*) \leq 0, \quad h_j(x^*) = 0$$

3. **DopuszczalnoÅ›Ä‡ dualna:**
$$\lambda_i^* \geq 0$$

4. **KomplementarnoÅ›Ä‡:**
$$\lambda_i^* g_i(x^*) = 0 \quad \forall i$$

```
KKT interpretacja geometryczna:

     âˆ‡f(x*)
        â†‘
        â”‚     âˆ‡gâ‚(x*)
        â”‚    â†—
   â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€  (granica ograniczenia)
        x*

W optimum: âˆ‡f = -Î»âˆ‡g (przeciwne kierunki, Î»â‰¥0)
```

---

### 4. Warunki regularnoÅ›ci (Constraint Qualification)

Warunki zapewniajÄ…ce, Å¼e KKT sÄ… konieczne:

| Warunek | Definicja |
|---------|-----------|
| **LICQ** (Linear Independence CQ) | Gradienty aktywnych ograniczeÅ„ liniowo niezaleÅ¼ne |
| **MFCQ** (Mangasarian-Fromovitz CQ) | SÅ‚absza wersja LICQ |
| **Slater** | Istnieje punkt Å›ciÅ›le wewnÄ…trz (dla wypukÅ‚ych) |

**LICQ:** $\{\nabla g_i(x^*) : g_i(x^*) = 0\} \cup \{\nabla h_j(x^*)\}$ sÄ… liniowo niezaleÅ¼ne

---

### 5. Warunki dostateczne II rzÄ™du

JeÅ›li speÅ‚nione KKT i dla hesjanu Lagrangianu:
$$d^T \nabla_{xx}^2 L(x^*, \lambda^*, \mu^*) d > 0$$

dla wszystkich $d \neq 0$ speÅ‚niajÄ…cych:
- $\nabla g_i(x^*)^T d = 0$ dla aktywnych $g_i$
- $\nabla h_j(x^*)^T d = 0$ dla wszystkich $h_j$

To $x^*$ jest **Å›cisÅ‚ym minimum lokalnym**.

---

### 6. Metody optymalizacji nieliniowej

#### Metody gradientowe (bez ograniczeÅ„)

| Metoda | Kierunek | ZbieÅ¼noÅ›Ä‡ |
|--------|----------|-----------|
| **Gradient Descent** | $d = -\nabla f$ | Liniowa |
| **Newton** | $d = -H^{-1}\nabla f$ | Kwadratowa |
| **BFGS** | Quasi-Newton | Superlinearna |
| **Conjugate Gradient** | SprzÄ™Å¼one kierunki | Superlinearna |

```
Gradient Descent:
x_{k+1} = x_k - Î±_k âˆ‡f(x_k)

Newton:
x_{k+1} = x_k - [âˆ‡Â²f(x_k)]^{-1} âˆ‡f(x_k)
```

#### Metody z ograniczeniami

| Metoda | Idea |
|--------|------|
| **Kary zewnÄ™trznej** | $\min f(x) + \rho \sum \max(0, g_i)^2$ |
| **Kary wewnÄ™trznej (Barrier)** | $\min f(x) - \mu \sum \log(-g_i)$ |
| **SQP** (Sequential Quadratic Programming) | Iteracyjne rozwiÄ…zywanie QP |
| **Interior Point** | Barrier + Newton |
| **Augmented Lagrangian** | Lagrangian + kara |

#### SQP - Algorytm

```
Repeat:
1. RozwiÄ…Å¼ podproblem QP:
   min  âˆ‡f(x_k)^T d + Â½ d^T H_k d
   s.t. g_i(x_k) + âˆ‡g_i(x_k)^T d â‰¤ 0
        h_j(x_k) + âˆ‡h_j(x_k)^T d = 0

2. x_{k+1} = x_k + Î±_k d_k

3. Aktualizuj H_k (BFGS)
```

---

### 7. PorÃ³wnanie metod

| Metoda | Ograniczenia | ZÅ‚oÅ¼onoÅ›Ä‡ iter. | ZbieÅ¼noÅ›Ä‡ |
|--------|--------------|-----------------|-----------|
| **Gradient** | Bez | O(n) | Liniowa |
| **Newton** | Bez | O(nÂ³) | Kwadratowa |
| **BFGS** | Bez | O(nÂ²) | Superlinearna |
| **SQP** | Z | O(nÂ³) per QP | Superlinearna |
| **Interior Point** | Z | O(nÂ³) | Polinomialna |

---

## ğŸ§  Mnemoniki

### "KKT = Keep Killing Troubles":
- **K**ondycja stacjonarnoÅ›ci (âˆ‡L = 0)
- **K**oniecznoÅ›Ä‡ dopuszczalnoÅ›ci (g â‰¤ 0, h = 0, Î» â‰¥ 0)
- **T**rick komplementarnoÅ›ci (Î»g = 0)

### "Hesjan dodatni = Minimum":
$H \succ 0$ â†’ punkt jest minimum (jak miska)

### "LICQ = Linear Independence":
Gradienty aktywnych ograniczeÅ„ muszÄ… byÄ‡ liniowo niezaleÅ¼ne

---

## â“ Pytania dodatkowe

### Q1: "Co oznacza warunek komplementarnoÅ›ci Î»áµ¢gáµ¢ = 0?"
**OdpowiedÅº:** Albo ograniczenie nieaktywne ($g_i < 0$, wtedy $\lambda_i = 0$), albo aktywne ($g_i = 0$, wtedy $\lambda_i \geq 0$). MnoÅ¼nik niezerowy tylko dla "ciasnych" ograniczeÅ„.

### Q2: "Kiedy KKT sÄ… warunkami dostatecznymi?"
**OdpowiedÅº:** Dla problemÃ³w wypukÅ‚ych (f wypukÅ‚a, g wypukÅ‚e, h liniowe). Wtedy kaÅ¼dy punkt KKT jest globalnym minimum.

### Q3: "Jaka jest przewaga BFGS nad Newtonem?"
**OdpowiedÅº:** BFGS nie wymaga obliczania hesjanu (tylko gradienty), przybliÅ¼a hesjan iteracyjnie. O(nÂ²) zamiast O(nÂ³) per iteracja. Bardziej odporny na nieÅ›cisÅ‚oÅ›ci.

---

## ğŸ¯ Kluczowe punkty

1. **Bez ograniczeÅ„:** $\nabla f = 0$ (konieczny), $H \succ 0$ (dostateczny)
2. **Z ograniczeniami:** KKT = stacjonarnoÅ›Ä‡ + dopuszczalnoÅ›Ä‡ + komplementarnoÅ›Ä‡
3. **RegularnoÅ›Ä‡:** LICQ, Slater - warunki na poprawnoÅ›Ä‡ KKT
4. **Metody:** Gradient, Newton, BFGS (bez), SQP, Interior Point (z)
5. **WypukÅ‚oÅ›Ä‡:** KKT konieczne i dostateczne

---

## ğŸ“– Å¹rÃ³dÅ‚a

1. Boyd, Vandenberghe - "Convex Optimization"
2. Nocedal, Wright - "Numerical Optimization"
3. Bazaraa et al. - "Nonlinear Programming"
