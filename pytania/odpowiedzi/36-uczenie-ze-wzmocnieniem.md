# Pytanie 36: Uczenie siÄ™ ze wzmocnieniem (Reinforcement Learning)

## Pytanie
**"OmÃ³wiÄ‡ ogÃ³lny algorytm, elementy skÅ‚adowe oraz wÅ‚asnoÅ›ci uczenia siÄ™ ze wzmocnieniem."**

Przedmiot: SIU (Systemy Inteligentne i UczÄ…ce siÄ™)

---

## ğŸ“š OdpowiedÅº gÅ‚Ã³wna

### 1. Model uczenia ze wzmocnieniem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              REINFORCEMENT LEARNING LOOP                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚          action aâ‚œ  â”‚             â”‚  reward râ‚œ                 â”‚
â”‚         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ ENVIRONMENT â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’                â”‚
â”‚         â”‚           â”‚             â”‚           â”‚                â”‚
â”‚         â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚                â”‚
â”‚         â”‚                 â”‚                   â”‚                â”‚
â”‚         â”‚          state sâ‚œâ‚Šâ‚                 â”‚                â”‚
â”‚         â”‚                 â”‚                   â”‚                â”‚
â”‚         â”‚                 â†“                   â†“                â”‚
â”‚         â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚    AGENT    â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                     â”‚  (policy Ï€) â”‚                            â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                                                 â”‚
â”‚  Cel: Maksymalizacja skumulowanej nagrody E[Î£ Î³áµ—râ‚œ]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2. Elementy skÅ‚adowe

| Element | Symbol | Opis |
|---------|--------|------|
| **State** | s âˆˆ S | Obserwacja Å›rodowiska |
| **Action** | a âˆˆ A | Decyzja agenta |
| **Reward** | r âˆˆ â„ | SygnaÅ‚ zwrotny |
| **Policy** | Ï€(a\|s) | Strategia wyboru akcji |
| **Value function** | V(s), Q(s,a) | Oczekiwana nagroda |
| **Discount factor** | Î³ âˆˆ [0,1] | WaÅ¼noÅ›Ä‡ przyszÅ‚ych nagrÃ³d |
| **Transition** | P(s'\|s,a) | Dynamika Å›rodowiska |

### Markov Decision Process (MDP)

```
MDP = (S, A, P, R, Î³)

S: ZbiÃ³r stanÃ³w
A: ZbiÃ³r akcji
P: P(s'|s,a) - prawdopodobieÅ„stwa przejÅ›Ä‡
R: R(s,a,s') - funkcja nagrody
Î³: WspÃ³Å‚czynnik dyskontowania

WÅ‚aÅ›ciwoÅ›Ä‡ Markowa:
  P(sâ‚œâ‚Šâ‚|sâ‚€,aâ‚€,...,sâ‚œ,aâ‚œ) = P(sâ‚œâ‚Šâ‚|sâ‚œ,aâ‚œ)
  
  PrzyszÅ‚oÅ›Ä‡ zaleÅ¼y tylko od obecnego stanu!
```

---

### 3. Funkcje wartoÅ›ci

#### State Value Function V(s)

$$V^\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s \right]$$

#### Action Value Function Q(s,a)

$$Q^\pi(s,a) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid s_0 = s, a_0 = a \right]$$

#### RÃ³wnania Bellmana

```
V*(s) = max_a [R(s,a) + Î³ Î£_s' P(s'|s,a) V*(s')]

Q*(s,a) = R(s,a) + Î³ Î£_s' P(s'|s,a) max_a' Q*(s',a')

Rekurencyjne rÃ³wnania - podstawa algorytmÃ³w DP
```

---

### 4. Algorytmy

#### 4.1 Value-based methods

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Q-LEARNING (off-policy, model-free)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]           â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                           TD target                             â”‚
â”‚                                                                 â”‚
â”‚ Algorytm:                                                       â”‚
â”‚ 1. Obserwuj stan s                                             â”‚
â”‚ 2. Wybierz akcjÄ™ a (Îµ-greedy)                                  â”‚
â”‚ 3. Wykonaj a, obserwuj r, s'                                   â”‚
â”‚ 4. Zaktualizuj Q(s,a)                                          â”‚
â”‚ 5. s â† s', goto 1                                              â”‚
â”‚                                                                 â”‚
â”‚ Îµ-greedy:                                                       â”‚
â”‚   Z prawdop. Îµ: losowa akcja (exploration)                     â”‚
â”‚   Z prawdop. 1-Îµ: argmax Q(s,a) (exploitation)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SARSA (on-policy)                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]                  â”‚
â”‚                              â†‘                                  â”‚
â”‚                    Rzeczywista nastÄ™pna akcja (nie max!)       â”‚
â”‚                                                                 â”‚
â”‚ RÃ³Å¼nica od Q-learning:                                          â”‚
â”‚ - UÅ¼ywa akcji faktycznie wybranej przez policy                 â”‚
â”‚ - Bardziej "ostroÅ¼ny" (uwzglÄ™dnia exploration)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.2 Deep Q-Network (DQN)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DQN - Q-learning z sieciÄ… neuronowÄ…                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ State s â†’ [Neural Network] â†’ Q(s,aâ‚), Q(s,aâ‚‚), ..., Q(s,aâ‚™)   â”‚
â”‚                                                                 â”‚
â”‚ Innowacje:                                                      â”‚
â”‚ 1. Experience Replay: bufor (s,a,r,s'), losowe prÃ³bkowanie    â”‚
â”‚ 2. Target Network: osobna sieÄ‡ do obliczania targetÃ³w         â”‚
â”‚    (aktualizowana co N krokÃ³w)                                 â”‚
â”‚                                                                 â”‚
â”‚ Loss: L = (r + Î³ max Q_target(s',a') - Q(s,a))Â²               â”‚
â”‚                                                                 â”‚
â”‚ PrzeÅ‚om: Atari games (2015)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.3 Policy-based methods

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POLICY GRADIENT                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ BezpoÅ›rednia optymalizacja parametrÃ³w Î¸ policy Ï€_Î¸(a|s)        â”‚
â”‚                                                                 â”‚
â”‚ Gradient:                                                       â”‚
â”‚ âˆ‡_Î¸ J(Î¸) = E_Ï€ [âˆ‡_Î¸ log Ï€_Î¸(a|s) Â· Q(s,a)]                    â”‚
â”‚                                                                 â”‚
â”‚ REINFORCE algorithm:                                            â”‚
â”‚ 1. Generuj trajektoriÄ™ Ï„ = (sâ‚€,aâ‚€,râ‚€,sâ‚,...)                  â”‚
â”‚ 2. Oblicz return: Gâ‚œ = Î£_{k=0}^{T-t} Î³áµ r_{t+k}               â”‚
â”‚ 3. Update: Î¸ â† Î¸ + Î± âˆ‡_Î¸ log Ï€_Î¸(aâ‚œ|sâ‚œ) Â· Gâ‚œ                  â”‚
â”‚                                                                 â”‚
â”‚ Zalety: ciÄ…gÅ‚e akcje, stochastic policies                     â”‚
â”‚ Wady: wysoka wariancja, wolna zbieÅ¼noÅ›Ä‡                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.4 Actor-Critic

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ACTOR-CRITIC - Å‚Ä…czy value-based i policy-based               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚     â”‚  ACTOR  â”‚ â†â”€â”€â”€â”€â”€ â”‚  CRITIC â”‚                             â”‚
â”‚     â”‚ Ï€_Î¸(a|s)â”‚        â”‚  V_w(s) â”‚                             â”‚
â”‚     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                             â”‚
â”‚          â”‚                  â”‚                                   â”‚
â”‚          â†“                  â”‚                                   â”‚
â”‚       action           value estimate                          â”‚
â”‚                                                                 â”‚
â”‚ Actor: wybiera akcje (policy)                                  â”‚
â”‚ Critic: ocenia jak dobre sÄ… akcje (value function)            â”‚
â”‚                                                                 â”‚
â”‚ Advantage: A(s,a) = Q(s,a) - V(s)                             â”‚
â”‚ Actor update: Î¸ â† Î¸ + Î± âˆ‡_Î¸ log Ï€_Î¸(a|s) Â· A(s,a)             â”‚
â”‚ Critic update: minimize (r + Î³V(s') - V(s))Â²                  â”‚
â”‚                                                                 â”‚
â”‚ Algorytmy: A2C, A3C, PPO, SAC                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5. Klasyfikacja algorytmÃ³w

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚               â”Œâ”€â”€ Model-based (zna/uczy siÄ™ P, R)              â”‚
â”‚   RL Methods â”€â”¤                                                 â”‚
â”‚               â””â”€â”€ Model-free (nie zna Å›rodowiska)              â”‚
â”‚                        â”‚                                        â”‚
â”‚                        â”œâ”€â”€ Value-based (Q-learning, DQN)       â”‚
â”‚                        â”œâ”€â”€ Policy-based (REINFORCE)            â”‚
â”‚                        â””â”€â”€ Actor-Critic (A2C, PPO, SAC)        â”‚
â”‚                                                                 â”‚
â”‚               â”Œâ”€â”€ On-policy (SARSA, A2C) - uÅ¼ywa obecnej Ï€     â”‚
â”‚   RL Methods â”€â”¤                                                 â”‚
â”‚               â””â”€â”€ Off-policy (Q-learning, DQN) - uÅ¼ywa innej Ï€ â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 6. Exploration vs Exploitation

| Strategia | Opis |
|-----------|------|
| **Îµ-greedy** | Z prawdop. Îµ losowa akcja |
| **Softmax/Boltzmann** | P(a) âˆ exp(Q(s,a)/Ï„) |
| **UCB** | a = argmax[Q(s,a) + câˆš(ln N / n(a))] |
| **Thompson Sampling** | PrÃ³bkowanie z posterior |
| **Curiosity-driven** | Bonus za nowoÅ›Ä‡ |

---

### 7. WÅ‚asnoÅ›ci i wyzwania

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WÅASNOÅšCI:                                                      â”‚
â”‚ âœ“ Uczenie przez interakcjÄ™ (nie supervised)                    â”‚
â”‚ âœ“ Delayed rewards (kredyt za sekwencjÄ™ akcji)                 â”‚
â”‚ âœ“ Generalizacja (do nowych stanÃ³w)                            â”‚
â”‚ âœ“ Online learning (ciÄ…gÅ‚e doskonalenie)                       â”‚
â”‚                                                                 â”‚
â”‚ WYZWANIA:                                                       â”‚
â”‚ âœ— Sample inefficiency (wymaga wielu interakcji)               â”‚
â”‚ âœ— Credit assignment (ktÃ³ra akcja odpowiada za reward?)        â”‚
â”‚ âœ— Exploration-exploitation tradeoff                           â”‚
â”‚ âœ— Partial observability (POMDP)                               â”‚
â”‚ âœ— Non-stationarity (Å›rodowisko siÄ™ zmienia)                  â”‚
â”‚ âœ— Reward shaping (sparse rewards problem)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Mnemoniki

### "SARSA = State Action Reward State Action":
Sekwencja w algorytmie SARSA

### "Q = Quality of action":
Q(s,a) mierzy jakoÅ›Ä‡ akcji a w stanie s

### "Actor-Critic = Action + Advice":
Actor wybiera akcje, Critic doradza (ocenia)

---

## â“ Pytania dodatkowe

### Q1: "Jaka jest rÃ³Å¼nica miÄ™dzy Q-learning a SARSA?"
**OdpowiedÅº:** Q-learning (off-policy): uÅ¼ywa max Q(s',a') - optymistyczne. SARSA (on-policy): uÅ¼ywa Q(s',a') gdzie a' to rzeczywista nastÄ™pna akcja - bardziej ostroÅ¼ne, uwzglÄ™dnia policy exploration.

### Q2: "Po co Experience Replay w DQN?"
**OdpowiedÅº:** Åamie korelacjÄ™ miÄ™dzy kolejnymi prÃ³bkami (iid requirement), efektywniejsze wykorzystanie danych (wielokrotne uczenie z jednej prÃ³bki), stabilizuje trening.

### Q3: "Kiedy model-based jest lepszy?"
**OdpowiedÅº:** Gdy Å›rodowisko jest przewidywalne, samples sÄ… drogie (robotyka), potrzebne planowanie. Model-free lepszy gdy Å›rodowisko zÅ‚oÅ¼one, trudne do modelowania, duÅ¼o samples dostÄ™pnych.

---

## ğŸ¯ Kluczowe punkty

1. **MDP:** (S, A, P, R, Î³) - formalizacja problemu
2. **Bellman:** V*(s) = max[R + Î³Â·Î£PÂ·V*]
3. **Q-learning:** off-policy, max Q update
4. **Policy Gradient:** âˆ‡J = E[âˆ‡log Ï€ Â· Q]
5. **Actor-Critic:** policy + value function

---

## ğŸ“– Å¹rÃ³dÅ‚a

1. Sutton, Barto - "Reinforcement Learning: An Introduction"
2. Silver - DeepMind RL lectures
3. Mnih et al. - "Playing Atari with Deep RL" (2015)
